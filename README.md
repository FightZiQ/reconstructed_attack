# Privacy Does Not Work: A Sample Reconstruction Attack against Federated Learning with Differential Privacy

**The implementation of paper:<br>
Privacy Does Not Work: A Sample Reconstruction Attack against Federated Learning with Differential Privacy**

We propose an attack against federated learning (FL) with differential privacy (DP) in which gradients uploaded by
users are clipped and perturbed.

Here are some examples:

* Original samples:
![original.png](fig%2Foriginal.png)

* Our reconstructed samples:
![res.png](fig%2Fres.png)

* Reconstructed samples generated by other attacks:
![when.png](fig%2Fwhen.png)

Other attacks are implemented by Jonas Geiping. If you are interested in more reconstruction samples in FL, please check
out his project: https://github.com/JonasGeiping/breaching

---

## Requirement

The following are the requirements for the implementation of this attack.

### 1. Dataset
We use four datasets in the experiments. The default dataset is ImageNet, please download it before you run this attack:
https://www.image-net.org/.

### 2. SAM

We apply SAM to training samples for masks generation and extract images' subjects before training. Please visit the
project website to get the latest `segment_anything`: https://github.com/facebookresearch/segment-anything.
Besides, you also need to download the latest SAM model checkpoint to your local devices.

### 3. Dependency path

After you download the dataset and SAM, please provide their local path in `dataset_path` and `sam_path` of `file_path.py`.
